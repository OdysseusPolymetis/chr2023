{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/chr2023/blob/main/5_example_ugarit_multilingualSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7DZEpdzDLu"
      },
      "source": [
        "# <center>**Measuring Greek and Latin similarity**</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUP8_s_FuAH9"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKrcW3eIdoZM",
        "outputId": "1ca67c4a-cf26-4e20-ac81-ab425aafc520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.7.0-py3-none-any.whl (933 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.2/933.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=bb47ac46b25271d355eec5d758dcf8d37b4d0f6e5bc1582ebebae9de1339d3b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: sentencepiece, emoji, stanza, pynndescent, umap-learn\n",
            "Successfully installed emoji-2.9.0 pynndescent-0.5.11 sentencepiece-0.1.99 stanza-1.7.0 umap-learn-0.5.5\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers numpy stanza umap-learn sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Skftlq81zZlz",
        "outputId": "f5c683aa-7868-4cd9-efcf-3dca2f2dee93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'canonical-greekLit'...\n",
            "remote: Enumerating objects: 60517, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 60517 (delta 50), reused 76 (delta 39), pack-reused 60422\u001b[K\n",
            "Receiving objects: 100% (60517/60517), 532.14 MiB | 15.77 MiB/s, done.\n",
            "Resolving deltas: 100% (40203/40203), done.\n",
            "Updating files: 100% (2631/2631), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PerseusDL/canonical-greekLit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwO2DZ8zpmZA",
        "outputId": "19857217-1e64-4591-e2d5-fb53cedb04a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'canonical-latinLit'...\n",
            "remote: Enumerating objects: 25683, done.\u001b[K\n",
            "remote: Counting objects: 100% (921/921), done.\u001b[K\n",
            "remote: Compressing objects: 100% (421/421), done.\u001b[K\n",
            "remote: Total 25683 (delta 515), reused 892 (delta 496), pack-reused 24762\u001b[K\n",
            "Receiving objects: 100% (25683/25683), 301.21 MiB | 15.88 MiB/s, done.\n",
            "Resolving deltas: 100% (16178/16178), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PerseusDL/canonical-latinLit.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlSKHcvvuVzD"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VKXQ_J3RzphS"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree\n",
        "import stanza\n",
        "from tqdm import tqdm\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Ck5s1-fr6-",
        "outputId": "315158cd-e976-498d-b59e-044b2084cf16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at UGARIT/grc-alignment were not used when initializing XLMRobertaForMaskedLM: ['psi_cls.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at UGARIT/grc-alignment were not used when initializing XLMRobertaForMaskedLM: ['psi_cls.bias', 'psi_cls.transform.weight', 'psi_cls.decoder.weight', 'psi_cls.transform.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForMaskedLM(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): XLMRobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BertModel\n",
        "pipe = pipeline(\"fill-mask\", model=\"UGARIT/grc-alignment\")\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"UGARIT/grc-alignment\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"UGARIT/grc-alignment\")\n",
        "model.eval()  # Mode évaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Mv0zot-AAd",
        "outputId": "ec5965ea-dc16-49db-aa9d-a6b25eb140cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁un', '▁exemple', '▁avec', '▁accent', '▁', 'ὡ', 'ἤ']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.tokenize(\"un exemple avec accent ὡἤ\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyuud1IkX3g"
      },
      "source": [
        "parcours des xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SSFlUoL-a-R",
        "outputId": "66fb0ec9-fa44-4c96-8956-2d14bebebc7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForMaskedLM(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): XLMRobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Aqa4Qgjb7Fzb"
      },
      "outputs": [],
      "source": [
        "def extract_body_content_from_xml(file_path):\n",
        "    parser = etree.XMLParser(recover=True)\n",
        "    tree = etree.parse(file_path, parser)\n",
        "    nsmap = tree.getroot().nsmap\n",
        "    default_ns = nsmap.get(None)\n",
        "\n",
        "    if default_ns:\n",
        "        body = tree.find(\".//ns:body\", namespaces={\"ns\": default_ns})\n",
        "    else:\n",
        "        body = tree.find(\".//body\")\n",
        "\n",
        "    if body is None:\n",
        "        raise ValueError(f\"No <body> element found in {file_path}\")\n",
        "\n",
        "    return etree.tostring(body, method=\"text\", encoding=\"unicode\")\n",
        "\n",
        "def is_latin(filename):\n",
        "    return re.search(r'lat\\d+\\.xml$', filename) is not None\n",
        "\n",
        "def is_greek(filename):\n",
        "    return re.search(r'grc\\d+\\.xml$', filename) is not None\n",
        "\n",
        "def extract_target_texts_from_directory(directory_path, target_authors):\n",
        "    texts = {}\n",
        "    for root, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.xml') and any(author_tag in file for author_tag in target_authors):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    content = extract_body_content_from_xml(file_path)\n",
        "                    texts[file] = content\n",
        "                except Exception as e:\n",
        "                    print(f\"Erreur lors du traitement du fichier {file_path}: {e}\")\n",
        "    return texts\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grc_target_authors = [\"tlg0059\",\"tlg0086\",\"tlg1325\", \"tlg0626\",\"tlg1304\",\"tlg0632\",\"tlg0591\",\"tlg0593\",\"tlg1562\",\"tlg1705\",\"tlg0014\",\"tlg0610\"]\n",
        "lat_target_authors = [\"phi0474\", \"phi1017\",\"stoa0255\",\"phi1014\",\"tlg0557\",\"phi0550\",\"tlg0628\",\"tlg0562\",\"phi1254\",\"phi1002\",\"stoa0058\",\"phi0684\",\"phi1212\"]\n"
      ],
      "metadata": {
        "id": "3oKuW7Y_xwkP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greek_corpus = extract_target_texts_from_directory('/content/canonical-greekLit', grc_target_authors)\n",
        "latin_corpus = extract_target_texts_from_directory('/content/canonical-latinLit', lat_target_authors)"
      ],
      "metadata": {
        "id": "g8F24MpKx6sv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def segment_corpus_into_sentences(corpus):\n",
        "    segmented_corpus = {}\n",
        "    for text_id, text in corpus.items():\n",
        "        segmented_corpus[text_id] = segment_into_sentences_with_index(text)\n",
        "    return segmented_corpus\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5wwcX_rIo07",
        "outputId": "9b0da9eb-d4c2-4a84-f00b-849cca53a48e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_into_sentences_with_index(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return {i: sentence for i, sentence in enumerate(sentences)}"
      ],
      "metadata": {
        "id": "wdfJl8BLJe6y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greek_corpus_segmented = segment_corpus_into_sentences(greek_corpus)\n",
        "latin_corpus_segmented = segment_corpus_into_sentences(latin_corpus)"
      ],
      "metadata": {
        "id": "KUoBeJiiIxYc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences_with_ugarit(sentences_dict, tokenizer, model, device, batch_size=32):\n",
        "    sentences_batches = [list(sentences_dict.values())[i:i + batch_size] for i in range(0, len(sentences_dict), batch_size)]\n",
        "\n",
        "    encoded_sentences = {}\n",
        "    sentence_index = 0\n",
        "\n",
        "    for batch in sentences_batches:\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        for i in range(outputs.hidden_states[-1].shape[0]):\n",
        "            sentence_embeddings = outputs.hidden_states[-1][i].mean(dim=0).squeeze(0)\n",
        "            encoded_sentences[sentence_index] = sentence_embeddings\n",
        "            sentence_index += 1\n",
        "\n",
        "    return encoded_sentences\n"
      ],
      "metadata": {
        "id": "HzacWxQmwxgl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_corpus_sentences(corpus_segmented, tokenizer, model, device, batch_size):\n",
        "    corpus_embeddings = {}\n",
        "    for text_id, sentences in tqdm(corpus_segmented.items(), desc=\"Encodage du corpus\"):\n",
        "        corpus_embeddings[text_id] = encode_sentences_with_ugarit(sentences, tokenizer, model, device, batch_size)\n",
        "    return corpus_embeddings"
      ],
      "metadata": {
        "id": "GW20ac3wJviS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "encoded_greek_corpus = encode_corpus_sentences(greek_corpus_segmented, tokenizer, model, device, batch_size)\n",
        "encoded_latin_corpus = encode_corpus_sentences(latin_corpus_segmented, tokenizer, model, device, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3hEWnUMJ4K5",
        "outputId": "a8f6a618-39ba-4cc3-b54f-aad2f5463106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encodage du corpus: 100%|██████████| 215/215 [08:55<00:00,  2.49s/it]\n",
            "Encodage du corpus:  61%|██████    | 94/154 [05:51<08:40,  8.67s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_sentence_similarity(greek_sentence_embeddings, latin_sentence_embeddings):\n",
        "    similarities = {}\n",
        "    for greek_id, greek_emb in tqdm(greek_sentence_embeddings.items(), desc=\"Calcul des similarités\"):\n",
        "        greek_emb_cpu = greek_emb.to('cpu').numpy()  # Déplacer sur le CPU et convertir en numpy\n",
        "        for latin_id, latin_emb in latin_sentence_embeddings.items():\n",
        "            latin_emb_cpu = latin_emb.to('cpu').numpy()  # Déplacer sur le CPU et convertir en numpy\n",
        "            sim = cosine_similarity([greek_emb_cpu], [latin_emb_cpu])[0][0]\n",
        "            similarities[(greek_id, latin_id)] = sim\n",
        "    return similarities\n"
      ],
      "metadata": {
        "id": "K9YiLEDSw6cZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_matches_for_each_text_pair(corpus_greek, corpus_latin, encoded_corpus_greek, encoded_corpus_latin):\n",
        "    best_matches = {}\n",
        "\n",
        "    for greek_text_id in corpus_greek:\n",
        "        for latin_text_id in corpus_latin:\n",
        "            greek_sentences = encoded_corpus_greek[greek_text_id]\n",
        "            latin_sentences = encoded_corpus_latin[latin_text_id]\n",
        "\n",
        "            # Calcul des similarités pour les paires de phrases\n",
        "            similarity_scores = calculate_sentence_similarity(greek_sentences, latin_sentences)\n",
        "\n",
        "            # Trouver la meilleure correspondance pour chaque paire de textes\n",
        "            best_match = max(similarity_scores.items(), key=lambda x: x[1])\n",
        "            best_matches[(greek_text_id, latin_text_id)] = best_match\n",
        "\n",
        "    return best_matches\n"
      ],
      "metadata": {
        "id": "KdHHbKm8Mhog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_matches = find_best_matches_for_each_text_pair(greek_corpus_segmented, latin_corpus_segmented, encoded_greek_corpus, encoded_latin_corpus)\n",
        "\n",
        "for ((greek_text_id, latin_text_id), ((greek_sentence_id, latin_sentence_id), sim_score)) in best_matches.items():\n",
        "    greek_sentence = greek_corpus_segmented[greek_text_id][greek_sentence_id]\n",
        "    latin_sentence = latin_corpus_segmented[latin_text_id][latin_sentence_id]\n",
        "\n",
        "    print(f\"Meilleure correspondance entre le texte grec {greek_text_id} et le texte latin {latin_text_id}:\")\n",
        "    print(f\"Phrase grecque: \\\"{greek_sentence}\\\"\")\n",
        "    print(f\"Phrase latine: \\\"{latin_sentence}\\\"\")\n",
        "    print(f\"Score de similarité: {sim_score}\\n\")\n"
      ],
      "metadata": {
        "id": "6XX4_DBw_3dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO20qvbTBxFuXWpq4tGALQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}